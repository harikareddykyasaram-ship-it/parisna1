# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbpr-dsrRBgJBsgis0jYOSBpbnk5Fxvo
"""

# Install required libraries
!pip install pymupdf faiss-cpu sentence-transformers gradio transformers

# Import necessary libraries
import fitz  # PyMuPDF for PDF text extraction
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import gradio as gr
import os

# Step 1: PDF Text Extraction using PyMuPDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text("text")
    return full_text

# Step 2: Semantic Search Using FAISS and SentenceTransformers
# Load sentence transformer model
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')

def create_embeddings(text):
    sentences = text.split("\n")  # Split text by sentences or paragraphs
    embeddings = sentence_model.encode(sentences, convert_to_tensor=True)
    return sentences, embeddings

# FAISS index setup
def setup_faiss_index(embeddings):
    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance index
    index.add(embeddings)  # Add embeddings to the FAISS index
    return index

def search_query(query, index, sentences, k=3):
    query_embedding = sentence_model.encode([query], convert_to_tensor=True)
    distances, indices = index.search(query_embedding, k)  # Search top k results
    results = [sentences[idx] for idx in indices[0]]
    return results

# Step 3: LLM-Based Answer Generation using IBM Granite 3.3 model from HuggingFace
llm_model = pipeline("text-generation", model="ibm/granite-3.3-2binstruct")

def generate_answer(query, context):
    prompt = f"Answer the following question based on the context:\n\nContext: {context}\n\nQuestion: {query}\nAnswer:"
    response = llm_model(prompt, max_length=200, num_return_sequences=1)
    return response[0]['generated_text']

# Step 4: Summarization Function
def summarize_text(text):
    prompt = f"Summarize the following text:\n{text}\nSummary:"
    summary = llm_model(prompt, max_length=200, num_return_sequences=1)
    return summary[0]['generated_text']

# Step 5: Quiz Mode (Interactive Study Mode)
def generate_quiz(text):
    prompt = f"Generate 5 multiple choice questions based on the following text:\n{text}\nQuestions:"
    questions = llm_model(prompt, max_length=200, num_return_sequences=1)
    return questions[0]['generated_text']

# Combine all functions for Gradio interface
def study_assistant(pdf_file, query, action="Q&A"):
    # Extract text from the PDF
    text = extract_text_from_pdf(pdf_file.name)

    # Step 2: Create embeddings and FAISS index
    sentences, embeddings = create_embeddings(text)
    index = setup_faiss_index(embeddings)

    # Handle different actions (Q&A, Summarization, Quiz Mode)
    if action == "Q&A":
        results = search_query(query, index, sentences)
        context = " ".join(results)  # Combine top search results for context
        answer = generate_answer(query, context)
        return answer

    elif action == "Summarization":
        summary = summarize_text(text)
        return summary

    elif action == "Quiz Mode":
        quiz = generate_quiz(text)
        return quiz

# Gradio Interface
def create_gradio_interface():
    # File upload and query input for user
    file_input = gr.inputs.File(label="Upload PDF")
    query_input = gr.inputs.Textbox(label="Enter Your Question")

    # Dropdown for selecting the action (Q&A, Summarization, Quiz Mode)
    action_dropdown = gr.inputs.Dropdown(choices=["Q&A", "Summarization", "Quiz Mode"], label="Select Action")

    # Gradio output to display the result
    output = gr.outputs.Textbox(label="Result")

    gr.Interface(fn=study_assistant, inputs=[file_input, query_input, action_dropdown], outputs=output).launch()

# Run the Gradio interface
create_gradio_interface()

#@title Studymate — single-cell Colab runnable app (PDF QA, FAISS, Granite LLM, Summarize, Quiz)
# Paste entire cell into a single Colab cell and run.
# You will be asked for a Hugging Face token. Use one with model download access.

# --- Install dependencies ---
import sys, os, textwrap, time
try:
    # Install necessary packages quietly
    !pip install -q transformers accelerate sentence-transformers faiss-cpu gradio pdfplumber "huggingface_hub>=0.14.0" safetensors
except:
    pass

# --- Imports ---
import io, math, json, threading
from huggingface_hub import login as hf_login
import pdfplumber
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import transformers
import gradio as gr
import torch

# --- User config ---
HF_MODEL = "ibm-granite/granite-3.3-2b-instruct"  # LLM for answer generation / summarization
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  # light, fast embeddings
CHUNK_SIZE = 1500   # characters per chunk (tune if needed)
CHUNK_OVERLAP = 200 # overlap characters between chunks
TOP_K = 5           # number of retrieved chunks for RAG
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Runtime device: {DEVICE}")

# --- Ask for HF token to download model ---
hf_token = input("Enter your Hugging Face token (kept in-memory only): ").strip()
if not hf_token:
    raise SystemExit("Hugging Face token is required to download the Granite model.")

hf_login(hf_token)

# --- Load embedding model (sentence-transformers) ---
print("Loading embedding model...")
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
embedder.max_seq_length = 512

# --- Helper: PDF text extraction ---
def extract_text_from_pdf_bytes(pdf_bytes):
    text_pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            # pdfplumber preserves layout & order well
            txt = page.extract_text(x_tolerance=1, y_tolerance=1) or ""
            text_pages.append(txt)
    full_text = "\n\n".join(text_pages)
    return full_text

# --- Helper: chunking text into overlapping chunks ---
def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    text = text.replace("\r\n", "\n")
    chunks = []
    start = 0
    N = len(text)
    while start < N:
        end = min(start + chunk_size, N)
        chunk = text[start:end]
        chunks.append(chunk.strip())
        if end == N:
            break
        start = max(0, end - overlap)
    return chunks

# --- Embedding + FAISS index build ---
class SemanticIndex:
    def __init__(self, embedder_model):
        self.embedder = embedder_model
        self.index = None
        self.texts = []   # original chunks
        self.metadatas = []  # optional metadata per chunk

    def build(self, chunks, metadatas=None):
        self.texts = chunks
        self.metadatas = metadatas or [{} for _ in chunks]
        print("Embedding chunks (may take a bit)...")
        embeddings = self.embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)
        d = embeddings.shape[1]
        # FAISS index
        self.index = faiss.IndexFlatIP(d)  # inner-product on normalized vectors -> cosine
        # normalize vectors
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        # store embeddings for re-normalization/search if needed
        self._embeddings = embeddings
        print(f"FAISS index built: {self.index.ntotal} vectors, dim={d}")

    def search(self, query, k=TOP_K):
        q_emb = self.embedder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, k)
        results = []
        for score, idx in zip(D[0], I[0]):
            if idx < 0:
                continue
            results.append({"score": float(score), "text": self.texts[idx], "meta": self.metadatas[idx]})
        return results

# --- Load the Granite LLM using transformers' pipeline ---
print(f"Loading Granite model ({HF_MODEL}) to device {DEVICE} — this may take a while and requires the HF token.")
# configure model loading to prefer fp16 on CUDA if available
model_kwargs = {}
if DEVICE == "cuda":
    model_kwargs.update({"torch_dtype": torch.float16, "device_map": "auto"})
else:
    model_kwargs.update({"device_map": "auto"})
# Use transformers AutoModelForCausalLM & AutoTokenizer
from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline

tokenizer = AutoTokenizer.from_pretrained(HF_MODEL, use_auth_token=hf_token, padding_side="left")
model = AutoModelForCausalLM.from_pretrained(HF_MODEL, use_auth_token=hf_token, trust_remote_code=True, **model_kwargs)
model.eval()
if DEVICE == "cuda":
    # ensure model is in half precision if CUDA
    try:
        model.half()
    except Exception:
        pass

generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0 if DEVICE=="cuda" else -1)

# --- Retrieval-augmented prompt helper ---
def build_rag_prompt(question, retrieved_chunks):
    instruction = (
        "You are a helpful assistant for academic study. Use the following extracted context from a student's PDF to answer the question.\n\n"
        "Context:\n"
    )
    ctx = "\n\n---\n\n".join([c["text"] for c in retrieved_chunks])
    prompt = f"{instruction}{ctx}\n\nQuestion: {question}\n\nAnswer concisely and accurately. If the answer is not fully contained in the context, say you don't have enough information and outline how to find it."
    return prompt

# --- LLM answer & summarization wrappers ---
def llm_generate(prompt, max_new_tokens=512, temperature=0.0, top_p=0.95):
    # generate text
    out = generator(prompt, max_new_tokens=max_new_tokens, do_sample=(temperature>0), temperature=temperature, top_p=top_p, eos_token_id=tokenizer.eos_token_id)
    return out[0]["generated_text"][len(prompt):].strip()

def answer_question(question, index: SemanticIndex):
    results = index.search(question, k=TOP_K)
    prompt = build_rag_prompt(question, results)
    ans = llm_generate(prompt, max_new_tokens=400, temperature=0.0)
    return {"answer": ans, "retrieved": results}

def summarize_text(text, style="brief"):
    # use model with an instruction to summarize
    prompt = f"Summarize the following text into a {style} summary suitable for a student studying the subject. Keep it accurate and list key bullet points.\n\nText:\n{text}\n\nSummary:\n"
    summary = llm_generate(prompt, max_new_tokens=300, temperature=0.0)
    return summary

def generate_quiz_from_text(text, num_questions=5):
    prompt = (
        "Create a short study quiz based on the following text. Produce exactly "
        f"{num_questions} multiple-choice questions. For each question provide: question text, 4 options labelled A-D, and the correct option letter. "
        "Also provide a 1-line explanation for the correct answer after the letter. Format as JSON list of objects with keys: question, options (list of 4), answer (letter), explanation.\n\n"
        f"Text:\n{text}\n\nQuiz JSON:\n"
    )
    out = llm_generate(prompt, max_new_tokens=800, temperature=0.2)
    # Try to find the JSON substring in the output
    try:
        # sometimes model returns extra text — find first '{' or '['
        start = out.find('[')
        if start != -1:
            json_text = out[start:]
        else:
            json_text = out
        quiz = json.loads(json_text)
    except Exception as e:
        # fallback: return plain text with message
        return {"error": "Could not parse JSON from model output. Raw output returned.", "raw": out}
    return quiz

# --- Global store for a loaded document ---
GLOBAL = {"index": None, "original_text": "", "chunks": [], "metadatas": []}

# --- Gradio UI functions ---
def on_upload(pdf_file):
    if pdf_file is None:
        return "No file uploaded.", None, None
    pdf_bytes = pdf_file.read()
    text = extract_text_from_pdf_bytes(pdf_bytes)
    if not text.strip():
        return "No text extracted from PDF.", None, None
    chunks = chunk_text(text)
    metadatas = [{"page_hint": i} for i in range(len(chunks))]
    idx = SemanticIndex(embedder)
    idx.build(chunks, metadatas=metadatas)
    GLOBAL["index"] = idx
    GLOBAL["original_text"] = text
    GLOBAL["chunks"] = chunks
    GLOBAL["metadatas"] = metadatas
    return f"PDF processed: {len(chunks)} chunks created.", f"{len(chunks)} chunks", text[:4000]

def on_ask(question):
    if not GLOBAL.get("index"):
        return "Please upload and process a PDF first.", "", ""
    res = answer_question(question, GLOBAL["index"])
    # show top retrieved snippets + answer
    retrieved_texts = "\n\n---\n\n".join([f"(score {r['score']:.3f})\n{r['text'][:800]}..." for r in res["retrieved"]])
    return res["answer"], retrieved_texts, question

def on_summarize(section_text, style_choice):
    if not section_text:
        # default to summarizing the entire doc (may be too long)
        section_text = GLOBAL.get("original_text","")
    if not section_text:
        return "No text available to summarize."
    summary = summarize_text(section_text, style=style_choice)
    return summary

def on_quiz(section_text, nq):
    if not section_text:
        section_text = GLOBAL.get("original_text","")
    if not section_text:
        return "No text available to generate quiz."
    quiz = generate_quiz_from_text(section_text, num_questions=int(nq))
    if isinstance(quiz, dict) and "error" in quiz:
        return f"Error generating quiz: {quiz['error']}\n\nRaw output:\n{quiz.get('raw')}"
    # format nicely
    out = ""
    for i,q in enumerate(quiz, start=1):
        out += f"{i}. {q.get('question')}\n"
        opts = q.get('options', [])
        for label,opt in zip(["A","B","C","D"], opts):
            out += f"   {label}. {opt}\n"
        out += f"Answer: {q.get('answer')} — {q.get('explanation')}\n\n"
    return out

# --- Build Gradio interface ---
with gr.Blocks(title="Studymate — PDF Academic Assistant") as demo:
    gr.Markdown("## Studymate\nUpload an academic PDF, ask conversational questions, get summaries, and launch quiz mode. Uses IBM Granite 3.3 2B Instruct for LLM tasks and FAISS for semantic search.")
    with gr.Row():
        pdf_in = gr.File(label="Upload PDF", file_types=['.pdf'])
        upload_btn = gr.Button("Process PDF")
    status = gr.Textbox(label="Status", interactive=False)
    chunks_info = gr.Textbox(label="Chunks info", interactive=False)
    preview = gr.Textbox(label="Text preview (first 4000 chars)", interactive=False)
    upload_btn.click(fn=lambda f: on_upload(f), inputs=[pdf_in], outputs=[status, chunks_info, preview])

    gr.Markdown("### Conversational Q&A")
    question = gr.Textbox(label="Ask a question about the uploaded PDF")
    ask_btn = gr.Button("Ask")
    answer_out = gr.Textbox(label="Answer")
    retrieved_out = gr.Textbox(label="Top retrieved snippets")
    ask_btn.click(fn=on_ask, inputs=[question], outputs=[answer_out, retrieved_out, question])

    gr.Markdown("### Summarization")
    section_text = gr.Textbox(label="Paste section/chapter text to summarize (optional). Leave empty to summarize whole doc.")
    style_choice = gr.Radio(choices=["brief","detailed","bullet points"], value="brief", label="Summary style")
    summarize_btn = gr.Button("Summarize")
    summary_out = gr.Textbox(label="Summary")
    summarize_btn.click(fn=on_summarize, inputs=[section_text, style_choice], outputs=[summary_out])

    gr.Markdown("### Interactive Study Mode — Quiz")
    quiz_text = gr.Textbox(label="Text to generate quiz from (optional). Leave empty to use full doc.")
    nq = gr.Slider(minimum=1, maximum=10, step=1, value=5, label="Number of questions")
    quiz_btn = gr.Button("Generate Quiz")
    quiz_out = gr.Textbox(label="Quiz")
    quiz_btn.click(fn=on_quiz, inputs=[quiz_text, nq], outputs=[quiz_out])

    gr.Markdown("### Tips")
    gr.Markdown("- For best results: upload clean, selectable-text PDFs (not scanned images). For scanned PDFs, OCR externally and re-upload text.\n- If model download fails due to memory limits, try switching to a smaller Granite variant or run locally with more RAM/GPU.")

# Launch Gradio app (in Colab it will appear as an iframe)
demo.launch(share=False, inbrowser=False)

#@title Studymate — single-cell Colab runnable app (PDF QA, FAISS, Granite LLM, Summarize, Quiz)
# Paste entire cell into a single Colab cell and run.
# You will be asked for a Hugging Face token. Use one with model download access.

# --- Install dependencies ---
import sys, os, textwrap, time
try:
    # Install necessary packages quietly
    !pip install -q transformers accelerate sentence-transformers faiss-cpu gradio pdfplumber "huggingface_hub>=0.14.0" safetensors
except:
    pass

# --- Imports ---
import io, math, json, threading
from huggingface_hub import login as hf_login
import pdfplumber
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import transformers
import gradio as gr
import torch

# --- User config ---
HF_MODEL = "ibm-granite/granite-3.3-2b-instruct"  # LLM for answer generation / summarization
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  # light, fast embeddings
CHUNK_SIZE = 1500   # characters per chunk (tune if needed)
CHUNK_OVERLAP = 200 # overlap characters between chunks
TOP_K = 5           # number of retrieved chunks for RAG
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Runtime device: {DEVICE}")

# --- Ask for HF token to download model ---
hf_token = input("Enter your Hugging Face token (kept in-memory only): ").strip()
if not hf_token:
    raise SystemExit("Hugging Face token is required to download the Granite model.")

hf_login(hf_token)

# --- Load embedding model (sentence-transformers) ---
print("Loading embedding model...")
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
embedder.max_seq_length = 512

# --- Helper: PDF text extraction ---
def extract_text_from_pdf_bytes(pdf_bytes):
    text_pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            # pdfplumber preserves layout & order well
            txt = page.extract_text(x_tolerance=1, y_tolerance=1) or ""
            text_pages.append(txt)
    full_text = "\n\n".join(text_pages)
    return full_text

# --- Helper: chunking text into overlapping chunks ---
def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    text = text.replace("\r\n", "\n")
    chunks = []
    start = 0
    N = len(text)
    while start < N:
        end = min(start + chunk_size, N)
        chunk = text[start:end]
        chunks.append(chunk.strip())
        if end == N:
            break
        start = max(0, end - overlap)
    return chunks

# --- Embedding + FAISS index build ---
class SemanticIndex:
    def __init__(self, embedder_model):
        self.embedder = embedder_model
        self.index = None
        self.texts = []   # original chunks
        self.metadatas = []  # optional metadata per chunk

    def build(self, chunks, metadatas=None):
        self.texts = chunks
        self.metadatas = metadatas or [{} for _ in chunks]
        print("Embedding chunks (may take a bit)...")
        embeddings = self.embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)
        d = embeddings.shape[1]
        # FAISS index
        self.index = faiss.IndexFlatIP(d)  # inner-product on normalized vectors -> cosine
        # normalize vectors
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        # store embeddings for re-normalization/search if needed
        self._embeddings = embeddings
        print(f"FAISS index built: {self.index.ntotal} vectors, dim={d}")

    def search(self, query, k=TOP_K):
        q_emb = self.embedder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, k)
        results = []
        for score, idx in zip(D[0], I[0]):
            if idx < 0:
                continue
            results.append({"score": float(score), "text": self.texts[idx], "meta": self.metadatas[idx]})
        return results

# --- Load the Granite LLM using transformers' pipeline ---
print(f"Loading Granite model ({HF_MODEL}) to device {DEVICE} — this may take a while and requires the HF token.")
# configure model loading to prefer fp16 on CUDA if available
model_kwargs = {}
if DEVICE == "cuda":
    model_kwargs.update({"torch_dtype": torch.float16, "device_map": "auto"})
else:
    model_kwargs.update({"device_map": "auto"})
# Use transformers AutoModelForCausalLM & AutoTokenizer
from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline

tokenizer = AutoTokenizer.from_pretrained(HF_MODEL, use_auth_token=hf_token, padding_side="left")
model = AutoModelForCausalLM.from_pretrained(HF_MODEL, use_auth_token=hf_token, trust_remote_code=True, **model_kwargs)
model.eval()
if DEVICE == "cuda":
    # ensure model is in half precision if CUDA
    try:
        model.half()
    except Exception:
        pass

generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0 if DEVICE=="cuda" else -1)

# --- Retrieval-augmented prompt helper ---
def build_rag_prompt(question, retrieved_chunks):
    instruction = (
        "You are a helpful assistant for academic study. Use the following extracted context from a student's PDF to answer the question.\n\n"
        "Context:\n"
    )
    ctx = "\n\n---\n\n".join([c["text"] for c in retrieved_chunks])
    prompt = f"{instruction}{ctx}\n\nQuestion: {question}\n\nAnswer concisely and accurately. If the answer is not fully contained in the context, say you don't have enough information and outline how to find it."
    return prompt

# --- LLM answer & summarization wrappers ---
def llm_generate(prompt, max_new_tokens=512, temperature=0.0, top_p=0.95):
    # generate text
    out = generator(prompt, max_new_tokens=max_new_tokens, do_sample=(temperature>0), temperature=temperature, top_p=top_p, eos_token_id=tokenizer.eos_token_id)
    return out[0]["generated_text"][len(prompt):].strip()

def answer_question(question, index: SemanticIndex):
    results = index.search(question, k=TOP_K)
    prompt = build_rag_prompt(question, results)
    ans = llm_generate(prompt, max_new_tokens=400, temperature=0.0)
    return {"answer": ans, "retrieved": results}

def summarize_text(text, style="brief"):
    # use model with an instruction to summarize
    prompt = f"Summarize the following text into a {style} summary suitable for a student studying the subject. Keep it accurate and list key bullet points.\n\nText:\n{text}\n\nSummary:\n"
    summary = llm_generate(prompt, max_new_tokens=300, temperature=0.0)
    return summary

def generate_quiz_from_text(text, num_questions=5):
    prompt = (
        "Create a short study quiz based on the following text. Produce exactly "
        f"{num_questions} multiple-choice questions. For each question provide: question text, 4 options labelled A-D, and the correct option letter. "
        "Also provide a 1-line explanation for the correct answer after the letter. Format as JSON list of objects with keys: question, options (list of 4), answer (letter), explanation.\n\n"
        f"Text:\n{text}\n\nQuiz JSON:\n"
    )
    out = llm_generate(prompt, max_new_tokens=800, temperature=0.2)
    # Try to find the JSON substring in the output
    try:
        # sometimes model returns extra text — find first '{' or '['
        start = out.find('[')
        if start != -1:
            json_text = out[start:]
        else:
            json_text = out
        quiz = json.loads(json_text)
    except Exception as e:
        # fallback: return plain text with message
        return {"error": "Could not parse JSON from model output. Raw output returned.", "raw": out}
    return quiz

# --- Global store for a loaded document ---
GLOBAL = {"index": None, "original_text": "", "chunks": [], "metadatas": []}

# --- Gradio UI functions ---
def on_upload(pdf_file):
    if pdf_file is None:
        return "No file uploaded.", None, None
    pdf_bytes = pdf_file.read()
    text = extract_text_from_pdf_bytes(pdf_bytes)
    if not text.strip():
        return "No text extracted from PDF.", None, None
    chunks = chunk_text(text)
    metadatas = [{"page_hint": i} for i in range(len(chunks))]
    idx = SemanticIndex(embedder)
    idx.build(chunks, metadatas=metadatas)
    GLOBAL["index"] = idx
    GLOBAL["original_text"] = text
    GLOBAL["chunks"] = chunks
    GLOBAL["metadatas"] = metadatas
    return f"PDF processed: {len(chunks)} chunks created.", f"{len(chunks)} chunks", text[:4000]

def on_ask(question):
    if not GLOBAL.get("index"):
        return "Please upload and process a PDF first.", "", ""
    res = answer_question(question, GLOBAL["index"])
    # show top retrieved snippets + answer
    retrieved_texts = "\n\n---\n\n".join([f"(score {r['score']:.3f})\n{r['text'][:800]}..." for r in res["retrieved"]])
    return res["answer"], retrieved_texts, question

def on_summarize(section_text, style_choice):
    if not section_text:
        # default to summarizing the entire doc (may be too long)
        section_text = GLOBAL.get("original_text","")
    if not section_text:
        return "No text available to summarize."
    summary = summarize_text(section_text, style=style_choice)
    return summary

def on_quiz(section_text, nq):
    if not section_text:
        section_text = GLOBAL.get("original_text","")
    if not section_text:
        return "No text available to generate quiz."
    quiz = generate_quiz_from_text(section_text, num_questions=int(nq))
    if isinstance(quiz, dict) and "error" in quiz:
        return f"Error generating quiz: {quiz['error']}\n\nRaw output:\n{quiz.get('raw')}"
    # format nicely
    out = ""
    for i,q in enumerate(quiz, start=1):
        out += f"{i}. {q.get('question')}\n"
        opts = q.get('options', [])
        for label,opt in zip(["A","B","C","D"], opts):
            out += f"   {label}. {opt}\n"
        out += f"Answer: {q.get('answer')} — {q.get('explanation')}\n\n"
    return out

# --- Build Gradio interface ---
with gr.Blocks(title="Studymate — PDF Academic Assistant") as demo:
    gr.Markdown("## Studymate\nUpload an academic PDF, ask conversational questions, get summaries, and launch quiz mode. Uses IBM Granite 3.3 2B Instruct for LLM tasks and FAISS for semantic search.")
    with gr.Row():
        pdf_in = gr.File(label="Upload PDF", file_types=['.pdf'])
        upload_btn = gr.Button("Process PDF")
    status = gr.Textbox(label="Status", interactive=False)
    chunks_info = gr.Textbox(label="Chunks info", interactive=False)
    preview = gr.Textbox(label="Text preview (first 4000 chars)", interactive=False)
    upload_btn.click(fn=lambda f: on_upload(f), inputs=[pdf_in], outputs=[status, chunks_info, preview])

    gr.Markdown("### Conversational Q&A")
    question = gr.Textbox(label="Ask a question about the uploaded PDF")
    ask_btn = gr.Button("Ask")
    answer_out = gr.Textbox(label="Answer")
    retrieved_out = gr.Textbox(label="Top retrieved snippets")
    ask_btn.click(fn=on_ask, inputs=[question], outputs=[answer_out, retrieved_out, question])

    gr.Markdown("### Summarization")
    section_text = gr.Textbox(label="Paste section/chapter text to summarize (optional). Leave empty to summarize whole doc.")
    style_choice = gr.Radio(choices=["brief","detailed","bullet points"], value="brief", label="Summary style")
    summarize_btn = gr.Button("Summarize")
    summary_out = gr.Textbox(label="Summary")
    summarize_btn.click(fn=on_summarize, inputs=[section_text, style_choice], outputs=[summary_out])

    gr.Markdown("### Interactive Study Mode — Quiz")
    quiz_text = gr.Textbox(label="Text to generate quiz from (optional). Leave empty to use full doc.")
    nq = gr.Slider(minimum=1, maximum=10, step=1, value=5, label="Number of questions")
    quiz_btn = gr.Button("Generate Quiz")
    quiz_out = gr.Textbox(label="Quiz")
    quiz_btn.click(fn=on_quiz, inputs=[quiz_text, nq], outputs=[quiz_out])

    gr.Markdown("### Tips")
    gr.Markdown("- For best results: upload clean, selectable-text PDFs (not scanned images). For scanned PDFs, OCR externally and re-upload text.\n- If model download fails due to memory limits, try switching to a smaller Granite variant or run locally with more RAM/GPU.")

# Launch Gradio app (in Colab it will appear as an iframe)
demo.launch(share=False, inbrowser=False)

#@title Studymate — single-cell Colab runnable app (PDF QA, FAISS, Granite LLM, Summarize, Quiz)
# Paste entire cell into a single Colab cell and run.
# You will be asked for a Hugging Face token. Use one with model download access.

# --- Install dependencies ---
import sys, os, textwrap, time
try:
    !pip install -q transformers accelerate sentence-transformers faiss-cpu gradio pdfplumber "huggingface_hub>=0.14.0" safetensors
except:
    pass

# --- Imports ---
import io, math, json, threading
from huggingface_hub import login as hf_login
import pdfplumber
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import transformers
import gradio as gr
import torch

# --- User config ---
HF_MODEL = "ibm-granite/granite-3.3-2b-instruct"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200
TOP_K = 5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Runtime device: {DEVICE}")

# --- Ask for HF token ---
hf_token = input("Enter your Hugging Face token (kept in-memory only): ").strip()
if not hf_token:
    raise SystemExit("Hugging Face token is required to download the Granite model.")

hf_login(hf_token)

# --- Load embedding model ---
print("Loading embedding model...")
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
embedder.max_seq_length = 512

# --- PDF text extraction ---
def extract_text_from_pdf_bytes(pdf_bytes):
    text_pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            txt = page.extract_text(x_tolerance=1, y_tolerance=1) or ""
            text_pages.append(txt)
    return "\n\n".join(text_pages)

# --- Text chunking ---
def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    text = text.replace("\r\n", "\n")
    chunks = []
    start = 0
    N = len(text)
    while start < N:
        end = min(start + chunk_size, N)
        chunks.append(text[start:end].strip())
        if end == N:
            break
        start = max(0, end - overlap)
    return chunks

# --- Embedding + FAISS index ---
class SemanticIndex:
    def __init__(self, embedder_model):
        self.embedder = embedder_model
        self.index = None
        self.texts = []
        self.metadatas = []

    def build(self, chunks, metadatas=None):
        self.texts = chunks
        self.metadatas = metadatas or [{} for _ in chunks]
        print("Embedding chunks...")
        embeddings = self.embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)
        d = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(d)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        self._embeddings = embeddings
        print(f"FAISS ready with {self.index.ntotal} vectors.")

    def search(self, query, k=TOP_K):
        q_emb = self.embedder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, k)
        results = []
        for score, idx in zip(D[0], I[0]):
            if idx >= 0:
                r

#@title Studymate — single-cell Colab runnable app (PDF QA, FAISS, Granite LLM, Summarize, Quiz)
# Paste entire cell into a single Colab cell and run.
# You will be asked for a Hugging Face token. Use one with model download access.

# --- Install dependencies ---
import sys, os, textwrap, time
try:
    !pip install -q transformers accelerate sentence-transformers faiss-cpu gradio pdfplumber "huggingface_hub>=0.14.0" safetensors
except:
    pass

# --- Imports ---
import io, math, json, threading
from huggingface_hub import login as hf_login
import pdfplumber
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import transformers
import gradio as gr
import torch

# --- User config ---
HF_MODEL = "ibm-granite/granite-3.3-2b-instruct"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200
TOP_K = 5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Runtime device: {DEVICE}")

# --- Ask for HF token ---
hf_token = input("Enter your Hugging Face token (kept in-memory only): ").strip()
if not hf_token:
    raise SystemExit("Hugging Face token is required to download the Granite model.")

hf_login(hf_token)

# --- Load embedding model ---
print("Loading embedding model...")
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
embedder.max_seq_length = 512

# --- PDF text extraction ---
def extract_text_from_pdf_bytes(pdf_bytes):
    text_pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            txt = page.extract_text(x_tolerance=1, y_tolerance=1) or ""
            text_pages.append(txt)
    return "\n\n".join(text_pages)

# --- Text chunking ---
def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    text = text.replace("\r\n", "\n")
    chunks = []
    start = 0
    N = len(text)
    while start < N:
        end = min(start + chunk_size, N)
        chunks.append(text[start:end].strip())
        if end == N:
            break
        start = max(0, end - overlap)
    return chunks

# --- Embedding + FAISS index ---
class SemanticIndex:
    def __init__(self, embedder_model):
        self.embedder = embedder_model
        self.index = None
        self.texts = []
        self.metadatas = []

    def build(self, chunks, metadatas=None):
        self.texts = chunks
        self.metadatas = metadatas or [{} for _ in chunks]
        print("Embedding chunks...")
        embeddings = self.embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)
        d = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(d)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        self._embeddings = embeddings
        print(f"FAISS ready with {self.index.ntotal} vectors.")

    def search(self, query, k=TOP_K):
        q_emb = self.embedder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, k)
        results = []
        for score, idx in zip(D[0], I[0]):
            if idx >= 0:
                results.append({"score": float(score), "text": self.texts[idx], "meta": self.metadatas[idx]})
        return results

# --- Load Granite LLM ---
print(f"Loading Granite model ({HF_MODEL})...")
model_kwargs = {"device_map": "auto"}
if DEVICE == "cuda":
    model_kwargs["torch_dtype"] = torch.float16

from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline

tokenizer = AutoTokenizer.from_pretrained(HF_MODEL, use_auth_token=hf_token, padding_side="left")
model = AutoModelForCausalLM.from_pretrained(HF_MODEL, use_auth_token=hf_token, trust_remote_code=True, **model_kwargs)
model.eval()

# --- FIXED: No device argument here ---
generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)

# --- RAG prompt ---
def build_rag_prompt(question, retrieved_chunks):
    ctx = "\n\n---\n\n".join(c["text"] for c in retrieved_chunks)
    return (
        "You are a helpful assistant. Use the context below to answer.\n\n"
        f"Context:\n{ctx}\n\n"
        f"Question: {question}\n\nAnswer:"
    )

# --- LLM utilities ---
def llm_generate(prompt, max_new_tokens=512, temperature=0.0, top_p=0.95):
    out = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=(temperature > 0),
        temperature=temperature,
        top_p=top_p,
        eos_token_id=tokenizer.eos_token_id
    )
    return out[0]["generated_text"][len(prompt):].strip()

def answer_question(question, index: SemanticIndex):
    retrieved = index.search(question)
    prompt = build_rag_prompt(question, retrieved)
    answer = llm_generate(prompt, max_new_tokens=400)
    return {"answer": answer, "retrieved": retrieved}

def summarize_text(text, style="brief"):
    prompt = (
        f"Summarize the following text into a {style} student-friendly summary.\n\n"
        f"Text:\n{text}\n\nSummary:\n"
    )
    return llm_generate(prompt, max_new_tokens=300)

def generate_quiz_from_text(text, num_questions=5):
    prompt = (
        "Create a quiz with exactly "
        f"{num_questions} MCQs in JSON format.\n\nText:\n{text}\n\nJSON:\n"
    )
    out = llm_generate(prompt, max_new_tokens=700, temperature=0.2)
    try:
        start = out.find('[')
        json_data = json.loads(out[start:]) if start != -1 else json.loads(out)
        return json_data
    except:
        return {"error": "Could not parse quiz JSON.", "raw": out}

# --- Global storage ---
GLOBAL = {"index": None, "original_text": "", "chunks": [], "metadatas": []}

# --- Gradio Functions ---
def on_upload(pdf_file):
    if pdf_file is None:
        return "No file uploaded.", None, None
    pdf_bytes = pdf_file.read()
    text = extract_text_from_pdf_bytes(pdf_bytes)
    if not text.strip():
        return "No text extracted.", None, None

    chunks = chunk_text(text)
    metadatas = [{"page_hint": i} for i in range(len(chunks))]

    idx = SemanticIndex(embedder)
    idx.build(chunks, metadatas)

    GLOBAL["index"] = idx
    GLOBAL["original_text"] = text
    GLOBAL["chunks"] = chunks
    GLOBAL["metadatas"] = metadatas

    return f"PDF processed. {len(chunks)} chunks.", str(len(chunks)), text[:4000]

def on_ask(question):
    if not GLOBAL["index"]:
        return "Upload a PDF first.", "", ""
    res = answer_question(question, GLOBAL["index"])
    retrieved_texts = "\n\n---\n\n".join(
        f"(score {r['score']:.3f})\n{r['text'][:500]}..."
        for r in res["retrieved"]
    )
    return res["answer"], retrieved_texts, question

def on_summarize(section_text, style):
    if not section_text:
        section_text = GLOBAL["original_text"]
    if not section_text:
        return "No text available."
    return summarize_text(section_text, style)

def on_quiz(section_text, nq):
    if not section_text:
        section_text = GLOBAL["original_text"]
    if not section_text:
        return "No text available."
    quiz = generate_quiz_from_text(section_text, int(nq))
    if "error" in quiz:
        return quiz["error"] + "\n\nRaw:\n" + quiz["raw"]
    out = ""
    for i, q in enumerate(quiz, 1):
        out += f"{i}. {q['question']}\n"
        for label, opt in zip("ABCD", q["options"]):
            out += f"   {label}. {opt}\n"
        out += f"Answer: {q['answer']} — {q['explanation']}\n\n"
    return out

# --- Gradio UI ---
with gr.Blocks(title="Studymate — PDF Academic Assistant") as demo:
    gr.Markdown("## Studymate — PDF Q&A • Summarizer • Quiz Generator")

    with gr.Row():
        pdf_in = gr.File(label="Upload PDF", file_types=[".pdf"])
        upload_btn = gr.Button("Process PDF")

    status = gr.Textbox(label="Status")
    chunks_info = gr.Textbox(label="Chunks info")
    preview = gr.Textbox(label="Preview (first 4000 chars)")

    upload_btn.click(on_upload, inputs=[pdf_in], outputs=[status, chunks_info, preview])

    gr.Markdown("### Ask Questions")
    question = gr.Textbox(label="Your question")
    ask_btn = gr.Button("Ask")
    answer_out = gr.Textbox(label="Answer")
    retrieved_out = gr.Textbox(label="Retrieved Context")
    ask_btn.click(on_ask, inputs=[question], outputs=[answer_out, retrieved_out, question])

    gr.Markdown("### Summarization")
    section_text = gr.Textbox(label="Paste text (optional)")
    style_choice = gr.Radio(["brief", "detailed", "bullet points"], value="brief", label="Style")
    summarize_btn = gr.Button("Summarize")
    summary_out = gr.Textbox(label="Summary")
    summarize_btn.click(on_summarize, inputs=[section_text, style_choice], outputs=[summary_out])

    gr.Markdown("### Quiz Mode")
    quiz_text = gr.Textbox(label="Text (optional)")
    nq = gr.Slider(1, 10, step=1, value=5, label="Number of questions")
    quiz_btn = gr.Button("Generate Quiz")
    quiz_out = gr.Textbox(label="Quiz")
    quiz_btn.click(on_quiz, inputs=[quiz_text, nq], outputs=[quiz_out])

demo.launch(share=False, inbrowser=False)